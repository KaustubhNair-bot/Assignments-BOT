# Tesla RAG System

A Retrieval-Augmented Generation (RAG) system for Tesla Policy and Product Knowledge Assistant.

## Overview

This enterprise-grade RAG system answers user queries using Tesla documents (PDFs), preventing hallucinations by grounding responses in retrieved context while maintaining Tesla's professional engineering and corporate tone.

## Architecture

```
Assignment-TamannaYadav/
â”œâ”€â”€ app/                 # Streamlit frontend
â”‚   â”œâ”€â”€ streamlit_app.py # Main Streamlit application
â”‚   â”œâ”€â”€ auth.py          # Mock authentication
â”‚   â””â”€â”€ ui_components.py # Tesla-themed UI components
â”œâ”€â”€ config/              # Configuration settings
â”œâ”€â”€ ingestion/           # PDF loading and text extraction
â”œâ”€â”€ preprocessing/       # Text cleaning and normalization
â”œâ”€â”€ chunking/            # Recursive text splitting
â”œâ”€â”€ embeddings/          # Embedding generation
â”œâ”€â”€ vector_db/           # FAISS index management
â”œâ”€â”€ retrieval/           # Similarity search
â”œâ”€â”€ prompts/             # Tesla persona templates
â”œâ”€â”€ rag/                 # RAG orchestration
â”œâ”€â”€ evaluation/          # Benchmarking and metrics
â”œâ”€â”€ utils/               # Helper functions
â”œâ”€â”€ data/                # Tesla PDF documents
â”œâ”€â”€ main.py              # CLI entry point
â””â”€â”€ requirements.txt     # Dependencies
```

## Installation

1. Clone the repository and navigate to the project:
```bash
cd tesla_rag_project
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
export GROQ_API_KEY="your-groq-api-key"
```

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Build the FAISS index (generates embeddings)
python scripts/build_index.py

# Run the evaluation
python -m evaluation.evaluate_rag

# Start the Streamlit app
streamlit run app/streamlit_app.py
```

---

## Usage

### Build the Index
First, build the FAISS index from Tesla documents:
```bash
python scripts/build_index.py
```

Or using main.py:
```bash
python main.py --mode build
```

To force rebuild:
```bash
python main.py --mode build --force
```

### Interactive Mode
Start an interactive session:
```bash
python main.py --mode interactive
```

### Single Query
Process a single query:
```bash
python main.py --mode query --query "What is Tesla's privacy policy?"
```

### Evaluation
Run evaluation on sample queries:
```bash
python main.py --mode evaluate
```

### Advanced Options
```bash
python main.py --mode interactive \
    --top-k 5 \
    --temperature 0.1 \
    --top-p 0.9
```

## Configuration

Key parameters in `config/settings.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| CHUNK_SIZE | 600 | Target chunk size in tokens |
| CHUNK_OVERLAP | 120 | Overlap between chunks |
| TOP_K | 5 | Number of chunks to retrieve |
| TEMPERATURE | 0.1 | LLM generation temperature |
| TOP_P | 0.9 | Top-p sampling parameter |
| SIMILARITY_THRESHOLD | 0.3 | Minimum similarity score |

## Components

### Document Ingestion
- Loads PDF files using `pdfplumber`
- Extracts text with page-level metadata
- Supports single files or directories

### Text Preprocessing
- Removes headers, footers, and noise
- Normalizes whitespace
- Preserves semantic structure

### Chunking
- Recursive character text splitting
- Configurable chunk size and overlap
- Maintains document metadata

### Embeddings
- Uses `sentence-transformers/all-MiniLM-L6-v2`
- 384-dimensional dense vectors
- Batch processing support

### Vector Database
- FAISS with cosine similarity (Inner Product)
- Persistent storage to disk
- Efficient similarity search

### Retrieval
- Query embedding generation
- Top-k similarity search
- Configurable threshold filtering

### Prompt Engineering
- Tesla brand persona
- Grounded response generation
- Hallucination prevention

### RAG Orchestration
- Combines retrieval and generation
- Groq LLM integration
- Configurable parameters

### Evaluation & Benchmarking
- Latency metrics (P50, P95, P99)
- Retrieval quality metrics
- JSON/CSV report generation
- RAG vs Non-RAG comparison
- Temperature experimentation

## Evaluation Framework

### Running Comprehensive Benchmarks
```bash
python main.py --mode benchmark
```

Or use the dedicated benchmark runner:
```bash
python evaluation/run_benchmark.py --mode all
```

### Benchmark Modes
| Mode | Description |
|------|-------------|
| `full` | Run all experiment configurations |
| `rag-comparison` | Compare RAG vs Non-RAG baseline |
| `temperature` | Test temperature variations |
| `all` | Run all benchmarks and generate report |

### Test Dataset
10 Tesla-related queries across 3 categories:

| Category | Count | Examples |
|----------|-------|----------|
| **Policy** | 3 | Service terms, dispute resolution |
| **Technical** | 4 | Safety features, charging, touchscreen |
| **Privacy/Legal** | 3 | Data collection, privacy rights |

### Metrics Computed

| Metric | Description |
|--------|-------------|
| **Relevance Score** | Semantic similarity between answer and retrieved chunks |
| **Groundedness Score** | % of answer sentences supported by context |
| **Hallucination Rate** | Proportion of unsupported claims |
| **Retrieval Precision** | Quality of top-k retrieved chunks |
| **Response Consistency** | Stability across multiple runs |

### Experiment Configurations

| Configuration | Temperature | Top-P | RAG |
|---------------|-------------|-------|-----|
| RAG_Factual | 0.0 | 0.9 | âœ“ |
| RAG_Balanced | 0.3 | 0.9 | âœ“ |
| RAG_Creative | 0.8 | 0.9 | âœ“ |
| RAG_HighTopP | 0.1 | 0.95 | âœ“ |
| NoRAG_Baseline | 0.1 | 0.9 | âœ— |

### Output Files
Benchmarks generate the following in `benchmark_results/`:
- `benchmark_YYYYMMDD_HHMMSS.json` - Full results
- `benchmark_YYYYMMDD_HHMMSS.csv` - Tabular results
- `BENCHMARK_REPORT.md` - Comprehensive analysis
- `benchmark_analysis.json` - Structured conclusions

---

## ğŸ“Š Evaluation Results - RAG vs Base LLM

### Summary Comparison

| Metric | RAG System | Base LLM | Winner | Improvement |
|--------|:----------:|:--------:|:------:|:-----------:|
| **Answer Relevance** | 0.980 | 1.000 | Tie | - |
| **Faithfulness** | 0.396 | 0.000 | **RAG** | âˆ |
| **Hallucination Risk** | 0.201 | 0.320 | **RAG** | **37% lower** |
| **ROUGE-L** | 0.215 | 0.000 | **RAG** | âˆ |
| **MRR** | 0.500 | 0.000 | **RAG** | âˆ |

### Key Finding

> **RAG system shows 37% lower hallucination risk** (0.201 vs 0.320) because answers are grounded in actual Tesla documents, not fabricated from training data.

### Why RAG Outperforms Base LLM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG vs Base LLM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  BASE LLM (Without RAG):                                         â”‚
â”‚  â”œâ”€â”€ Relies only on training data (knowledge cutoff)            â”‚
â”‚  â”œâ”€â”€ May hallucinate specific details                           â”‚
â”‚  â”œâ”€â”€ Generic knowledge                                          â”‚
â”‚  â””â”€â”€ No grounding in actual documents                           â”‚
â”‚                                                                  â”‚
â”‚  RAG SYSTEM (With Retrieval):                                    â”‚
â”‚  â”œâ”€â”€ Retrieves relevant chunks from Tesla PDFs                  â”‚
â”‚  â”œâ”€â”€ Answers grounded in real documents                         â”‚
â”‚  â”œâ”€â”€ Cites specific sources                                     â”‚
â”‚  â””â”€â”€ 37% lower hallucination risk                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Metrics Explained

| Metric | What It Measures | Why It Matters |
|--------|------------------|----------------|
| **Answer Relevance** | Does the answer address the query? | Basic quality check |
| **Faithfulness** | Is the answer grounded in retrieved context? | RAG's key advantage |
| **Hallucination Risk** | Does the response contain fabricated info? | Critical for enterprise use |
| **ROUGE-L** | Text overlap with source documents | Measures grounding quality |
| **MRR** | Rank of first relevant retrieved document | Retriever effectiveness |

---

### Key Findings

#### Why RAG Reduces Hallucinations
1. **Context Grounding** - Retrieval provides factual anchors
2. **Explicit Instructions** - Persona prompt forbids fabrication
3. **Source Verification** - Retrieved chunks enable fact-checking
4. **Reduced Parametric Reliance** - Uses current documents vs training data

#### Why Low Temperature Improves Accuracy
1. **Deterministic Sampling** - Selects most probable tokens
2. **Consistent Outputs** - Reproducible responses
3. **Reduced Exploration** - Stays close to high-confidence predictions

#### Tesla Persona Impact
1. **Role Definition** - Sets professional expectations
2. **Explicit Constraints** - Prevents hallucination
3. **Fallback Behavior** - Handles uncertainty gracefully
4. **Tone Consistency** - Maintains brand voice

## Streamlit Frontend

### Running the Web Application

Start the Streamlit app:
```bash
streamlit run app/streamlit_app.py
```

The app will open at `http://localhost:8501`

### Login Credentials (Demo)

| Username | Password |
|----------|----------|
| `tesla_admin` | `tesla123` |
| `demo_user` | `demo123` |

### Frontend Features

1. **Mock Authentication**
   - Secure login page
   - Session management
   - Logout functionality

2. **Chat Interface**
   - Real-time query processing
   - Conversation history
   - Tesla-styled responses

3. **Retrieved Context Panel**
   - View top-k retrieved chunks
   - Source document names
   - Similarity scores
   - Verify answer grounding

4. **Generation Controls (Sidebar)**
   - Temperature slider (0.0 - 1.0)
   - Top-P slider (0.5 - 1.0)
   - Top-K dropdown (3, 5, 7, 10)

5. **Tesla-Themed UI**
   - Clean, minimalistic design
   - Tesla brand colors
   - Professional layout

### Frontend Structure

```
app/
â”œâ”€â”€ streamlit_app.py    # Main application
â”œâ”€â”€ auth.py             # Authentication module
â””â”€â”€ ui_components.py    # Reusable UI components
```

## API Keys

This system requires a Groq API key for LLM inference:
1. Sign up at [Groq Console](https://console.groq.com)
2. Generate an API key
3. Set as environment variable: `export GROQ_API_KEY="your-key"`

## License

This project is for educational purposes.
